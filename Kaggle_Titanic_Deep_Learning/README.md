
# Titanic Kaggle Competition

In this latest attempt of the Titanic Kaggle Competition a small Neural Network was constructed using a linear activation function for the first dense layer, and for the second dense layer a activation function of a rectified linear unit, and the output layer a sigmiod activation function. Additionally the loss of the function was best used as binary crossentropy, and the best performing optimizer was NAdam. However, I did attempt to increase my feature engineering, however, it improved my score a little. My new score is 0.78468 in Kaggle.





## Acknowledgements

 - Professors at Kean University
 - Professors at NJIT
 - Research Mentors at Kean University
 - StackOverflow Q&A Discussion
 - ChatGPT
 - ClaudeAI


## Authors

- [@FPyC639](https://github.com/FPyC639)


## Appendix

[!["Buy Me A Coffee"](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://www.buymeacoffee.com/joseserra8x)
